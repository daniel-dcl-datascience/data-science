{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with Convolutional Neural Networks\n",
    "\n",
    "The following model aims to predict wether an amazon review is positive or negative. The product reviews are from the Home and Kitchen section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the required packages are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import keras as ks\n",
    "from matplotlib import pyplot\n",
    "import gzip \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the data is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path): \n",
    "    g = gzip.open(path, 'rb') \n",
    "    for l in g: \n",
    "        yield eval(l) \n",
    "def getDF(path): \n",
    "    i = 0 \n",
    "    df = {} \n",
    "    for d in parse(path): \n",
    "        df[i] = d \n",
    "        i += 1 \n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "df = getDF('reviews_Home_and_Kitchen_5.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APYOBQE6M18AA</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Martin Schwartz</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Price</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>10 19, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JVQTAGHYOL7F</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Michelle Dinh</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I bought this zoku quick pop for my daughterr ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>zoku</td>\n",
       "      <td>1403049600</td>\n",
       "      <td>06 18, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3UPYGJKZ0XTU4</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>mirasreviews</td>\n",
       "      <td>[26, 27]</td>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "      <td>1367712000</td>\n",
       "      <td>05 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2MHCTX43MIMDZ</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>M. Johnson \"Tea Lover\"</td>\n",
       "      <td>[14, 18]</td>\n",
       "      <td>This book is a must have if you get a Zoku (wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Creative Combos</td>\n",
       "      <td>1312416000</td>\n",
       "      <td>08 4, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHAI85T5C2DH3</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>PugLover</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This cookbook is great.  I have really enjoyed...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A must own if you own the Zoku maker...</td>\n",
       "      <td>1402099200</td>\n",
       "      <td>06 7, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin            reviewerName   helpful  \\\n",
       "0   APYOBQE6M18AA  0615391206         Martin Schwartz    [0, 0]   \n",
       "1  A1JVQTAGHYOL7F  0615391206           Michelle Dinh    [0, 0]   \n",
       "2  A3UPYGJKZ0XTU4  0615391206            mirasreviews  [26, 27]   \n",
       "3  A2MHCTX43MIMDZ  0615391206  M. Johnson \"Tea Lover\"  [14, 18]   \n",
       "4   AHAI85T5C2DH3  0615391206                PugLover    [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  My daughter wanted this book and the price on ...      5.0   \n",
       "1  I bought this zoku quick pop for my daughterr ...      5.0   \n",
       "2  There is no shortage of pop recipes available ...      4.0   \n",
       "3  This book is a must have if you get a Zoku (wh...      5.0   \n",
       "4  This cookbook is great.  I have really enjoyed...      4.0   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                         Best Price      1382140800   \n",
       "1                                               zoku      1403049600   \n",
       "2  Excels at Sweet Dessert Pops, but Falls Short ...      1367712000   \n",
       "3                                    Creative Combos      1312416000   \n",
       "4            A must own if you own the Zoku maker...      1402099200   \n",
       "\n",
       "    reviewTime  \n",
       "0  10 19, 2013  \n",
       "1  06 18, 2014  \n",
       "2   05 5, 2013  \n",
       "3   08 4, 2011  \n",
       "4   06 7, 2014  "
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable will be based on the overall rating column, therefore, samples with that missing variable are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['overall'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to label as positive review those with rating above 3 and as negative those below 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['overall']!=3.0]\n",
    "df.loc[df['overall']>3.0, 'sent'] = 1\n",
    "df.loc[df['overall']<3.0, 'sent'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary and reviewText are concatenated such that there is only one input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['summary'] + '. ' + df['reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check again the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>sent</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APYOBQE6M18AA</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Martin Schwartz</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Price</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>10 19, 2013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Price. My daughter wanted this book and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JVQTAGHYOL7F</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Michelle Dinh</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I bought this zoku quick pop for my daughterr ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>zoku</td>\n",
       "      <td>1403049600</td>\n",
       "      <td>06 18, 2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>zoku. I bought this zoku quick pop for my daug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3UPYGJKZ0XTU4</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>mirasreviews</td>\n",
       "      <td>[26, 27]</td>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "      <td>1367712000</td>\n",
       "      <td>05 5, 2013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2MHCTX43MIMDZ</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>M. Johnson \"Tea Lover\"</td>\n",
       "      <td>[14, 18]</td>\n",
       "      <td>This book is a must have if you get a Zoku (wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Creative Combos</td>\n",
       "      <td>1312416000</td>\n",
       "      <td>08 4, 2011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Creative Combos. This book is a must have if y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHAI85T5C2DH3</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>PugLover</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This cookbook is great.  I have really enjoyed...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A must own if you own the Zoku maker...</td>\n",
       "      <td>1402099200</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A must own if you own the Zoku maker.... This ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin            reviewerName   helpful  \\\n",
       "0   APYOBQE6M18AA  0615391206         Martin Schwartz    [0, 0]   \n",
       "1  A1JVQTAGHYOL7F  0615391206           Michelle Dinh    [0, 0]   \n",
       "2  A3UPYGJKZ0XTU4  0615391206            mirasreviews  [26, 27]   \n",
       "3  A2MHCTX43MIMDZ  0615391206  M. Johnson \"Tea Lover\"  [14, 18]   \n",
       "4   AHAI85T5C2DH3  0615391206                PugLover    [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  My daughter wanted this book and the price on ...      5.0   \n",
       "1  I bought this zoku quick pop for my daughterr ...      5.0   \n",
       "2  There is no shortage of pop recipes available ...      4.0   \n",
       "3  This book is a must have if you get a Zoku (wh...      5.0   \n",
       "4  This cookbook is great.  I have really enjoyed...      4.0   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                         Best Price      1382140800   \n",
       "1                                               zoku      1403049600   \n",
       "2  Excels at Sweet Dessert Pops, but Falls Short ...      1367712000   \n",
       "3                                    Creative Combos      1312416000   \n",
       "4            A must own if you own the Zoku maker...      1402099200   \n",
       "\n",
       "    reviewTime  sent                                             review  \n",
       "0  10 19, 2013   1.0  Best Price. My daughter wanted this book and t...  \n",
       "1  06 18, 2014   1.0  zoku. I bought this zoku quick pop for my daug...  \n",
       "2   05 5, 2013   1.0  Excels at Sweet Dessert Pops, but Falls Short ...  \n",
       "3   08 4, 2011   1.0  Creative Combos. This book is a must have if y...  \n",
       "4   06 7, 2014   1.0  A must own if you own the Zoku maker.... This ...  "
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best Price. My daughter wanted this book and the price on Amazon was the best.  She has already tried one recipe a day after receiving the book.  She seems happy with it.'"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the variables that are not of interest are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['review','sent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split into train and test dataset at 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what is the balance between postive and negative classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    364192\n",
       "0.0     41106\n",
       "Name: sent, dtype: int64"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sent.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is and unbalanced dataset. As the computing resources are limited and the amount of data large enough, we are going to randomly drop a number of postive cases until the dataset is balanced. That will help with common class inbalance issues and help with the undestanding of accuracy measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random under-sampling:\n",
      "0.0    41106\n",
      "1.0    41106\n",
      "Name: sent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Class count\n",
    "count_class_0 = (train['sent'] == 0).sum()\n",
    "count_class_1 = (train['sent'] == 1).sum()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = train[train['sent'] == 0]\n",
    "df_class_1 = train[train['sent'] == 1]\n",
    "df_class_1_under = df_class_1.sample(count_class_0)\n",
    "train = pd.concat([df_class_1_under, df_class_0], axis=0)\n",
    "print('Random under-sampling:')\n",
    "print(train.sent.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check now the number of unique words in the resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82211\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(np.hstack(train.review))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we expand the contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train['review'] = train['review'].apply(contractions.fix)\n",
    "test['review'] = test['review'].apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can NOT recommend. these dish cloths worked great for me at first but after several washings they are falling apart.  i am having the same problems with the towels.  do not waste your money.'"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['review'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now we will tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train['review'] = train['review'].apply(nltk.word_tokenize)\n",
    "test['review'] = test['review'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can',\n",
       " 'NOT',\n",
       " 'recommend',\n",
       " '.',\n",
       " 'these',\n",
       " 'dish',\n",
       " 'cloths',\n",
       " 'worked',\n",
       " 'great',\n",
       " 'for',\n",
       " 'me',\n",
       " 'at',\n",
       " 'first',\n",
       " 'but',\n",
       " 'after',\n",
       " 'several',\n",
       " 'washings',\n",
       " 'they',\n",
       " 'are',\n",
       " 'falling',\n",
       " 'apart',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'having',\n",
       " 'the',\n",
       " 'same',\n",
       " 'problems',\n",
       " 'with',\n",
       " 'the',\n",
       " 'towels',\n",
       " '.',\n",
       " 'do',\n",
       " 'not',\n",
       " 'waste',\n",
       " 'your',\n",
       " 'money',\n",
       " '.']"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['review'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to do a set of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "train['review'] = train['review'].apply(normalize)\n",
    "test['review'] = test['review'].apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the number of unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93566"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['review'].apply(pd.Series).stack().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the number of words in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 46.44 words STD(32.682022)\n"
     ]
    }
   ],
   "source": [
    "train['totalwords'] = train['review'].str.len().copy()\n",
    "print(\"Mean %.2f words STD(%f)\" % (np.mean(train['totalwords']), np.std(train['totalwords'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.boxplot(train['totalwords'])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the 10% most extreme samples in terms of number of words such that we have a better look at the number of words in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['totalwords']>train['totalwords'].quantile(0.05)]\n",
    "train = train[train['totalwords']<train['totalwords'].quantile(0.95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAC6FJREFUeJzt3XGonfddx/H3x8ZublKTLLelJsVUCNM5lI1LqQ5kLILdHEv+WKEyNMxAEIpOJ9hW/8j8b0Nx6h8OwlobodSVOmkRFUPsKILtuNnG1jbThI6l19bmjKSbOFCjX/+4T9ylvbc3Oc+5Pck37xeEc57nPM8533/yvg+/e849qSokSX1937wHkCRtLkMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJam5LfMeAGDHjh21e/fueY8hSVeVEydOfKuqFjY67ooI/e7du1laWpr3GJJ0VUnyzUs5zqUbSWrO0EtSc4ZekprbMPRJHkhyNskzq/b9fpKvJ/lqkr9KsnXVY/clOZ3kn5P8/GYNLkm6NJdyRf8gcMer9h0D3llVPwn8C3AfQJJ3AHcBPzGc86dJrpvZtJKky7Zh6KvqSeDcq/b9fVVdGDafAnYN9/cBf1FV/1lV3wBOA7fNcF7pDZPkNf+kq9Es1uh/Bfjb4f5O4IVVjy0P+14jyaEkS0mWJpPJDMaQZme9qBt7XY1GhT7J7wIXgIcu7lrjsDW/q7CqjlTVYlUtLixs+H5/SdKUpv7AVJIDwAeBvfW9L55dBm5Zddgu4MXpx5MkjTXVFX2SO4B7gA9V1XdXPfQ4cFeSNyW5FdgDfHH8mJKkaW14RZ/kYeC9wI4ky8BhVt5l8ybg2LBm+VRV/WpVPZvkEeA5VpZ07q6q/9ms4SVJG8v3Vl3mZ3FxsfxbN7qSvN4vXa+E/zMSQJITVbW40XF+MlaSmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJam5qb94RLoazeKrAC/lOfwLl7qSGHpdUy4nwGsF3YDramTopXVcjHoSA6+rmmv0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzW0Y+iQPJDmb5JlV+7YnOZbk1HC7bdifJH+S5HSSryZ592YOL0na2KVc0T8I3PGqffcCx6tqD3B82AZ4P7Bn+HcI+MxsxpQkTWvD0FfVk8C5V+3eBxwd7h8F9q/a/+e14ilga5KbZzWsJOnyTbtGf1NVvQQw3N447N8JvLDquOVh32skOZRkKcnSZDKZcgxJ0kZm/cvYtb5Mc82v5qmqI1W1WFWLCwsLMx5DknTRtKF/+eKSzHB7dti/DNyy6rhdwIvTjydJGmva0D8OHBjuHwAeW7X/l4d339wOfPviEo8kaT42/HLwJA8D7wV2JFkGDgOfBB5JchA4A9w5HP43wAeA08B3gY9uwsySpMuwYeir6hfXeWjvGscWcPfYoSRJs+MnYyWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaGxX6JL+Z5NkkzyR5OMmbk9ya5Okkp5J8Lsn1sxpWknT5pg59kp3ArwOLVfVO4DrgLuBTwKerag9wHjg4i0ElSdMZu3SzBfiBJFuAtwAvAe8DHh0ePwrsH/kakqQRpg59Vf0r8AfAGVYC/23gBPBKVV0YDlsGdq51fpJDSZaSLE0mk2nHkCRtYMzSzTZgH3Ar8MPAW4H3r3ForXV+VR2pqsWqWlxYWJh2DEnSBsYs3fwc8I2qmlTVfwOfB34G2Dos5QDsAl4cOaMkaYQxoT8D3J7kLUkC7AWeA54APjwccwB4bNyIkqQxxqzRP83KL12/BHxteK4jwD3Ax5OcBt4G3D+DOSVJU9qy8SHrq6rDwOFX7X4euG3M80qSZsdPxkpSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Nyr0SbYmeTTJ15OcTPLTSbYnOZbk1HC7bVbDSpIu39gr+j8G/q6qfgz4KeAkcC9wvKr2AMeHbUnSnEwd+iQ3AD8L3A9QVf9VVa8A+4Cjw2FHgf1jh5QkTW/MFf2PAhPgz5J8Oclnk7wVuKmqXgIYbm9c6+Qkh5IsJVmaTCYjxpAkvZ4xod8CvBv4TFW9C/gPLmOZpqqOVNViVS0uLCyMGEOS9HrGhH4ZWK6qp4ftR1kJ/8tJbgYYbs+OG1GSNMbUoa+qfwNeSPL2Ydde4DngceDAsO8A8NioCSVJo2wZef6vAQ8luR54HvgoKz88HklyEDgD3DnyNaQ1bd++nfPnz78hr5VkU59/27ZtnDt3blNfQ9euUaGvqq8Ai2s8tHfM80qX4vz581TVvMeYic3+QaJrm5+MlaTmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWpudOiTXJfky0n+eti+NcnTSU4l+VyS68ePKUma1iyu6D8GnFy1/Sng01W1BzgPHJzBa0iSpjQq9El2Ab8AfHbYDvA+4NHhkKPA/jGvIUkaZ+wV/R8Bvw3877D9NuCVqrowbC8DO9c6McmhJEtJliaTycgxJEnrmTr0ST4InK2qE6t3r3ForXV+VR2pqsWqWlxYWJh2DEnSBraMOPc9wIeSfAB4M3ADK1f4W5NsGa7qdwEvjh9TkjStqa/oq+q+qtpVVbuBu4B/qKqPAE8AHx4OOwA8NnpKSdLUNuN99PcAH09ympU1+/s34TUkSZdozNLN/6uqLwBfGO4/D9w2i+eVJI3nJ2MlqTlDL0nNzWTpRpqHOnwDfOKH5j3GTNThG+Y9ghoz9Lpq5fe+Q9WaH9O46iShPjHvKdSVSzeS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnNb5j2ANEaSeY8wE9u2bZv3CGrM0OuqVVVvyOskecNeS9oMUy/dJLklyRNJTiZ5NsnHhv3bkxxLcmq49VJFkuZozBr9BeC3qurHgduBu5O8A7gXOF5Ve4Djw7YkaU6mDn1VvVRVXxru/ztwEtgJ7AOODocdBfaPHVKSNL2ZvOsmyW7gXcDTwE1V9RKs/DAAblznnENJlpIsTSaTWYwhSVrD6NAn+UHgL4HfqKrvXOp5VXWkqharanFhYWHsGJKkdYwKfZLvZyXyD1XV54fdLye5eXj8ZuDsuBElSWOMeddNgPuBk1X1h6seehw4MNw/ADw2/XiSpLHGvI/+PcAvAV9L8pVh3+8AnwQeSXIQOAPcOW5ESdIYU4e+qv4RWO9jiXunfV5J0mz5t24kqTlDL0nNGXpJas7QS1Jz/vVKXVOm/bPGl3uef+1SVxJDr2uKAda1yKUbSWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nN5Ur4AEmSCfDNec8hrWMH8K15DyGt4UeqasPvYr0iQi9dyZIsVdXivOeQpuXSjSQ1Z+glqTlDL23syLwHkMZwjV6SmvOKXpKaM/TSOpI8kORskmfmPYs0hqGX1vcgcMe8h5DGMvTSOqrqSeDcvOeQxjL0ktScoZek5gy9JDVn6CWpOUMvrSPJw8A/AW9Pspzk4LxnkqbhJ2MlqTmv6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNfd/DbxmKsPZMfUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.boxplot(train['totalwords'])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check if the dataset is still balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    33473\n",
      "1.0    32465\n",
      "Name: sent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.sent.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Lets finally resent the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sent</th>\n",
       "      <th>totalwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[quiet, effective, aesthetic, new, air, purifi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[really, sleek, excellent, item, really, enjoy...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[knockoff, made, china, real, kitchenaid, buy,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[second, gelpro, mat, long, days, kitchen, hol...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[design, needs, improvement, item, holds, lid,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sent  totalwords\n",
       "0  [quiet, effective, aesthetic, new, air, purifi...   1.0          54\n",
       "1  [really, sleek, excellent, item, really, enjoy...   1.0          22\n",
       "2  [knockoff, made, china, real, kitchenaid, buy,...   0.0          52\n",
       "3  [second, gelpro, mat, long, days, kitchen, hol...   1.0          38\n",
       "4  [design, needs, improvement, item, holds, lid,...   0.0          65"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to detokenize now such that we can take advantage of Keras tokenizer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "train['review'] = train['review'].apply(TreebankWordDetokenizer().detokenize)\n",
    "test['review'] = test['review'].apply(TreebankWordDetokenizer().detokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tokenize now with Keras tokenizer and use the top 500 most common words for word sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence\n",
    "vocabulary_size = 500\n",
    "t = Tokenizer(num_words= vocabulary_size)\n",
    "t.fit_on_texts(train['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad dataset to a maximum review length in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ... 346 268  90]\n",
      " [  0   0   0 ...  23   4   8]\n",
      " [  0   0   0 ... 302 487 107]\n",
      " ...\n",
      " [  0   0   0 ...  97  39 105]\n",
      " [  0   0   0 ... 236 302 432]\n",
      " [  0   0   0 ...   5  46 237]]\n"
     ]
    }
   ],
   "source": [
    "max_words = 100\n",
    "sequences_train = t.texts_to_sequences(train['review'])\n",
    "X_train = ks.preprocessing.sequence.pad_sequences(sequences_train, maxlen=max_words)\n",
    "sequences_test = t.texts_to_sequences(test['review'])\n",
    "X_test = ks.preprocessing.sequence.pad_sequences(sequences_test, maxlen=max_words)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compile the LSTM neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_43 (Embedding)     (None, 100, 32)           16000     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 50)                16600     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 32,651\n",
      "Trainable params: 32,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 32 \n",
    "model = Sequential() \n",
    "model.add(Embedding(top_words, \n",
    "                    embedding_vector_length, \n",
    "                    input_length=max_words)) \n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(units=50,\n",
    "               activation='tanh',\n",
    "               recurrent_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 32651 parameters to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65938 samples, validate on 101325 samples\n",
      "Epoch 1/3\n",
      "65938/65938 [==============================] - 237s 4ms/step - loss: 0.3825 - acc: 0.8289 - val_loss: 0.2427 - val_acc: 0.9050\n",
      "Epoch 2/3\n",
      "65938/65938 [==============================] - 231s 4ms/step - loss: 0.3153 - acc: 0.8642 - val_loss: 0.3246 - val_acc: 0.8554\n",
      "Epoch 3/3\n",
      "65938/65938 [==============================] - 236s 4ms/step - loss: 0.3035 - acc: 0.8692 - val_loss: 0.3181 - val_acc: 0.8678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x249f5e926a0>"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, \n",
    "          train.sent, \n",
    "          validation_data=(X_test, test.sent), \n",
    "          epochs=3, \n",
    "          batch_size=128,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models is trained and we have achieved a 87% accuracy in the validation dataset within a few minutes of training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source citation:\n",
    "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering\n",
    "R. He, J. McAuley\n",
    "WWW, 2016"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 193.85,
   "position": {
    "height": "40px",
    "left": "568px",
    "right": "28px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
